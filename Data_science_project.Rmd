---
title: "Regression & Clustering methods for Football Games results predictions"
output:
  pdf_document:
    keep_tex: yes
    toc: yes
    toc_depth: 1
    fig_width: 6 
    fig_height: 4
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '1'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r pressure, echo=FALSE, fig.align="center", out.width = '60%'}
knitr::include_graphics("aalto.png")
#fig.cap="A caption"
```

```{r, results='asis', echo=FALSE}
#cat("\\newpage")
```

```{r, echo=FALSE, cache=FALSE, results='hyde', warning=FALSE, comment=FALSE, warning=FALSE}
football_test_x <- read.csv("football_test_x.csv")
football_test_y <- read.csv("football_test_y.csv")
football_train_x <- read.csv("football_train_x.csv")
football_train_y <- read.csv("football_train_y.csv")
football_x=rbind(football_train_x, football_test_x)
football_y=rbind(football_train_y, football_test_y)
football_train=cbind(football_train_x, football_train_y)
```

```{r, results='asis', echo=FALSE}
#cat("\\newpage")
```

#1 Abstract
The aim of this project is to analyze football games data, in order to understand which features drive the most the final score of a football match, to predict the number of goals and to classify a match as interesting (i.e. if final goal difference is greater or equal to 3).
For the first task multiple Regression Models are used. For the second one, Classification Mdels are used: K-Nearest-Neighbour (KNN) and Logistic Regression.
For both of the tasks, the models are first applied to all the features and then to most relevant the principal components, given by the principal component analysis. 
The models which best predict are the linear model and the KNN, both applied to all the features.



#2 Introduction
The given dataset is from the Premier League in the UK in seasons 2016,2017 and 2018. The data were collected and distributed by Football-Data.co.uk. 
Python and R are the languages used to explore the data and to select the most relevant regression and classification methods for the given tasks. In the end, the pdf has been generated by using RMarkdown.
The aim of this project is to get insightful knowledge about the given datasets so as to be able to perform the best possible predictions. Furthermore, our personal motivation is to understand how to explore a given dataset, how to apply and compare models of regression and classification. 



#3 Data Analysis

##3.1 Dataset description

The used 13 features from the original dataframe are:

 * “HomeTeam", “AwayTeam": names of the home and away teams, mapped between 0 and 19;
 * “HTHG", “HTAG": half time goals of home and away teams;
 * “HTR": half time results, 1, if home team won, -1 if it lost, 0 for a draw;
 * “HS", “AS": number of shots by home and away teams.
 * “HST", “AST": number of shots on target by home and away teams;
 * “HF", “AF": number of fouls by home and away teams;
 * “HC", “AC": number of corners of home and away teams;

The used train-set contains 798 observations, and the test-set contains 342 examples. The split is thus around 70%-30%.
The data present two labels: the total number of goals (FTG) and the interesting class distribution (Interest). 
The Distributions of FTG and Interest among the training set are plotted.

```{r, echo=FALSE, out.width='.49\\linewidth', fig.show='hold', fig.height = 3, fig.width = 3, fig.align="center"}
library(ggplot2)
ggplot(data=football_y, aes(football_y$FTG)) + 
  geom_histogram(aes(y = ..density..), 
                 breaks=seq(0, 10, by = 1), 
                 col="lightsalmon", 
                 fill="lightsalmon4", 
                 alpha = .2) + 
  geom_density(col="lightsalmon3") + 
  labs(title="Distribution of FTG") +
  labs(x="FTG", y="Count" )

ggplot(data=football_y, aes(football_y$Interest)) + 
  geom_histogram(aes(y = ..density..), 
                 breaks=seq(0, 1, by = 0.5), 
                 col="deepskyblue", 
                 fill="deepskyblue4", 
                 alpha = .2) + 
  geom_density(col="deepskyblue3") + 
  labs(title="Distribution of Interest") +
  labs(x="Interest", y="Count" )
```


    
##3.2 Correlation
The first step of the data analysis is to plot the correlation between all pairwise features to visualize which ones are the most correlated one another. This analysis is done on the training dataset. The higher the value, the higher the correlation. Since the Correlation Matrix is symmetric, just the upper part is plotted. The blue colour is used for positive correlation (+1), the red colour for negative (-1). The numbers in between follow the gradient, as displayed in the legend on the right.


```{r, echo=FALSE, fig.height = 9, fig.width = 9, size="tiny", comment=NA}
library(corrplot)
library(ROCR)
M <- cor(football_train)
col<- colorRampPalette(c("red", "pink", "blue"))(30)
corrplot(M, method="number", bg="white", type="upper", tl.col="black", col=col)
```

The following strongest correlations are observed, with the correlation value: 

* 0.71 - HS and HST: number of shots and number of shots on target by home teams;
* 0.68 - AS and AST: number of shots and number of shots on target by away teams;
* 0.67 - HTR and HTHG: half time results and half time goals of home teams;
* 0.58 - HC and HS: number of corners and number of shots by home teams;
* 0.55 - AC and AS: number of corners and number of shots by away teams;

##3.3 Pair plots
The pair plots between these strongly correlated features are plotted.
For each plot, the blue colour is used for non interesting matches (Interest=0) while the orange colour for interesting matches (Interest=1).
```{r, echo=FALSE, out.width='.49\\linewidth', fig.show='hold', fig.height = 5, fig.width = 5, size="tiny"}
my_cols <- c("#00AFBB", "#E7B800")  #, "#FC4E07"
plot1=cbind("HS"=football_x$HS, "HC"=football_x$HC, "HST"=football_x$HST)
plot2=cbind("AS"=football_x$AS, "AC"=football_x$AC, "AST"=football_x$AST)
plot3=cbind("HTR"=football_x$HTR, "HTHG"=football_x$HTHG)
pairs(plot1, pch = 19,  cex = 1,
      col = my_cols[football_y$Interest+1],
      lower.panel=NULL)
pairs(plot2, pch = 19,  cex = 1,
      col = my_cols[football_y$Interest+1],
      lower.panel=NULL)
```

```{r, echo=FALSE, fig.height = 3, fig.width = 3, size="tiny", fig.align="center"}
pairs(plot3, pch = 19,  cex = 1,
      col = my_cols[football_y$Interest+1],
      lower.panel=NULL)

```

##3.4 Principal component analysis
The principal component analysis is done on the training dataset.
In the following plot, the data are projected on the plane generated by the first two principal components.

```{r, echo=FALSE}
tourists <- football_train_x
n <- dim(tourists)[1]
p <- dim(tourists)[2]
pc.tourists <- princomp(tourists, scores=T)
#summary(pc.tourists)
```

```{r, echo=FALSE, fig.height = 3, fig.width = 5,fig.align="center"}
plot(pc.tourists$scores[,1],pc.tourists$scores[,2], xlab="Principal component 1", ylab="Principal component 2", main="Football train x in PC1-PC2 plane", col="lightblue")
```

The cumulative percentage of variance explained by the first four principal components is:

* First principal cpmponent: 26.55%
* Second principal component: 50.55%
* Third principal component: 74.12%
* Fourth principal component: 81.50%

The istograms of the explained variance are plotted on the left; above, for the original features and below for the principal component analysis. The cumulative explained variance is plotted on the right.

```{r, echo=FALSE, fig.height = 4, fig.width = 6, fig.align="center"}
layout(matrix(c(2,3,1,3),2,byrow=T))
plot(pc.tourists, las=2, main='Principal components', col="lightblue")
barplot(sapply(tourists,sd)^2, las=2, main='Original Variables', ylab='Variances', col="blue")
plot(cumsum(pc.tourists$sd^2)/sum(pc.tourists$sd^2), type='b', axes=F, xlab='number of components', 
     ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(tourists),labels=1:ncol(tourists),las=2)
```

In order to reduce the dimensionality of the original dataset (i.e. the number of the features), just some principal components could be kept: in this case the first four principal components could be considered enough to explain the majority of the variability (at least 80%) of the original dataset.
The principal components are, by definition, a linear combination of the original features. The loadings ($e_{i,k}$ i=1,..,13 (features), k=1,..,4 (principal components) ), i.e. the weights of this linear combinations, can be also analyzed in order to understand which of the original features contribuite the most in the definition of the principal components. 
$$  PC_1= e_{1,1}\cdot HomeTeam + ... + e_{13,1}\cdot AC $$
$$  PC_2= e_{1,2}\cdot HomeTeam + ... + e_{13,2}\cdot AC $$
$$  PC_3= e_{1,3}\cdot HomeTeam + ... + e_{13,3}\cdot AC $$
$$  PC_4= e_{1,4}\cdot HomeTeam + ... + e_{13,4}\cdot AC $$
In the following plot, the barplots indicate the weight of the features for the first four principal components (one for each line).

```{r, echo=FALSE, fig.height = 6, fig.width = 9, fig.align="center"}
par(mar = c(1,4,0,2), mfrow = c(4,1))
for(i in 1:4) barplot(ifelse(abs(pc.tourists$loadings[,i]) < 0.1, 0, pc.tourists$loadings[,i]) , ylim = c(-1, 1.2), col="lightblue");abline(h=0)
```

#4 Methods

##4.1 Regression

##4.1.1 Regression with all the features
In this section the focus is on Regression Models analysis to predict the number of goals of the match (FTG feature) based on all or some of the features.

#### Fitting of the model on train dataset
A linear regression model is first applied on the train dataset. The model consists in finding coefficients $\beta_i$ and $\sigma$ such that the labels (here FTG is y) are explained as linear combination of all the features (noted $x_k$ here, and k in [1,n]). 
$$\hat{y} = \beta_0 + \beta_1\cdot x_1+ \beta_2\cdot x_2+...+ \beta_n \cdot x_n +\epsilon  $$

Then a quadratic regression model is taken into account. Here the squared features are considered($x_k^2$). 
$$\hat{y} = \beta_0 + \beta_1 \cdot x_1^2+ \beta_2\cdot x_2^2+...+ \beta_n \cdot x_n^2 +\epsilon  $$

Finally, the linear and the quadratic mode are combined: 
$$\hat{y} = \beta_0 + \beta_{11}\cdot x_1+ \beta_{12} \cdot x_1^2+\beta_{21} \cdot x_2+ \beta_{22} \cdot x_2^2+...+\epsilon  $$

For all the above models, $\beta_0$ is the intercept and $\epsilon$ is the error, normally distributed $\epsilon \sim N(0,\sigma^2)$.

For all the above regression models the p-values of the coefficients is checked , by testing 
$$ H_0: \beta_i = 0 \ \ \ \ vs \ \ \ \ H_1:\beta_i \neq 0  $$.
If the p-value is higher than 5%, the models will be refitted removing that coefficient (below referred as reduced models).

#### Measures of goodness of the models
The coefficient of determination ($R^2$) is taken into account to analyze the goodness of the model, i.e. the proportion of the variance in the dependent variable that is predictable from the independent variables $x_k$.
It is a statistical measure of how well the regression predictions approximate the real data points. An $R^2$ equal to 1 indicates that the regression predictions perfectly fit the data (which also indicates that the model is not good: overfitting is not a good prediction).
Thus, the higher and closer to 1 (but not exactly 1), the better.

The residual standard error (RSE) is also analyzed for each of the fitted model. It is a measure of the variability of the residuals from a linear model.

#### Prediction on the test dataset
After the computation of the model based on the training dataset and the calculation of $\beta_i$ and $\sigma$, the fitted model is used to predict the number of goals per match in the test dataset; the following measures are computed: mean error (ME), the error rate (mean error over the mean FTG), and the mean-squared error (MSE). 
$$ ME = \sum(y - \hat{y})$$
$$ ME_{ratio} = \frac{ME}{\overline{y}}$$
$$ MSE = \sqrt{\sum(y - \hat{y})^2}$$
Where ^y is the predicted FTG, and y the true label FTG.
These three values are taken into account for comparing the predictions obtained by the fitted models.
^y, y  are then plotted together on the same plot and analyzed.


##4.1.2 Regression with Principal Component Analysis
In this section, the PCA is adopted to reduce the dimension of the dataset. Thus the regression models are applied on the principal components. The principal component analysis is performed on the training dataset.
By the analysis of the contribution to the cumulative variance and the rule of the 80% at least, the first four principal components are taken into account, as explained in section 3.4.

Two kinds of regressions are performed on the train dataset:
Linear regression:
$$ \hat{\bar{y}} = \beta_0 + \beta_1 \cdot PC_1 + \beta_2\cdot PC_2 + \beta_3\cdot PC_3 + \beta_4 \cdot PC_4 +\bar{\epsilon} $$
and mixed (linear and quadratic) regression:

$$ \hat{\bar{y}} = \beta_0 + \beta_1 \cdot PC_1 + \beta_2\cdot PC_2 + \beta_3\cdot PC_3 + \beta_4 \cdot PC_4 +
\beta_5 \cdot PC_1^2 + \beta_6\cdot PC_2^2 + \beta_7\cdot PC_3^2 + \beta_8 \cdot PC_4^2 + \epsilon $$

with $$ \epsilon \sim N(0,\bar{\sigma}^2)$$

All the steps and indexes adopted are the same of the above section.
The only difference is that after the calculation of $\beta_i$ and $\sigma$, the model must be restransformed in the space of the original features, in order to get ^y: the prediction of FTG comparable with the results obrained in the previous section.
This can be done by rewriting the Principal components as linear combination of the original featues and by the use of the loadings ($e_{i,k}$, i=1,..,13 (features), k=1,..,4 (principal components)) (explained in section 3.4)
$$ \hat{y} = \beta_0 + \beta_1 \cdot ( e_{1,1}\cdot HomeTeam + ... + e_{13,1}\cdot AC) + ... \beta_1 \cdot ( e_{1,4}\cdot HomeTeam + ... + e_{13,4}\cdot AC) + \epsilon $$
This procedure is implemented in the code and y^ and y  are in the end plotted together on the same plot and analyzed.


##4.2 Classification
The aim of this section is to classify the football games as interesting or not, using different methods: KNN and Logistic Regression.

###4.2.1 KNN with all features
First the K-Nearest-Neighbours method is used to classify the data.
The algorithm is said to be a lazy-algorithm. For a given point, it looks for the k training points the nearest (computing distances with all trained data points) and clusters the given points according to the cluster of the majority of the k neighbors. 
We first look for the optimal number of neighbors to take into account, optimal by minimizing the error of clustering. For this the algorithm is run with values of k from 1 to 40. We compute the error for each k: 
$$ error  \ of \ misclassification= \frac{number\ of\ misclassified \ samples}{all \ samples} $$
k is chosen as the index at which the error is minimum. In fact, the lower the error of misclassification is, the better.
For this k, the confusion matrix is calculated and plotted.
Then, the Accuracy is calculated as
$$  Accuracy = 1- error  \ of \ misclassification$$
Of course, the higher the Accuracy, the better.

###4.2.2 KNN with Principal Component Analysis
In orer to reduce the dimensionality of the data, the PCA analysis is run again on the whole dataset so as to select only some features (the principal components that explain enough variance) on which we will cluster with the KNN method.
We aim at using enough Principal Components to explain at least 80% of the explained variance of the model.
We run a PCA analysis on the combined dataset: the train and test sets of features (train x and test x) together, that we will divide again later for clustering purposes. 
The used indexes for the analysis of the goodness of the model are the same as the previous section.

####4.2.3 Logistic Regression Classification with all features
In this section, the logistic regression is used for classification of a match as interesting or not.
The logistic regression (or logit regression) estimates the parameters of a logistic model (a form of binary regression). The dependent variable with two possible values ("0" and "1": not interesting and interesting). In the logistic model, the log-odds (the logarithm of the odds) for the value labeled "1" is a linear combination of one or more independent variables ("predictors").
Let $p=P(Y=1)$, where Y is the response variable (Interest).
$$ log_b(\frac{p}{1-p}) = \beta_0 + \beta_1\cdot HomeTeam + ... +  \beta_{13} \cdot AC $$
$$ \frac{p}{1-p} = b^{\beta_0 + \beta_1\cdot HomeTeam + ... +  \beta_{13} \cdot AC }$$
$$ p = \frac{b^{\beta_0 + \beta_1\cdot HomeTeam + ... +  \beta_{13} \cdot AC }}{ 1 + b^{\beta_0 + \beta_1\cdot HomeTeam + ... +  \beta_{13} \cdot AC }}$$


The confusion matrix is calculated and plotted. Then, the misclassification error and the accuracy are calculated as in section 4.2.1.
Moreover, the ROC curve is plotted and the AUC value is calculated.
AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. 

####4.2.4 Logistic Regression Classification with Principal Component Analysis
The same procedure of 4.2.3 is used in order to get a prediction of the Interest on a match, but this time it is performed on the Principal components in the same way as described in section 4.2.2.
Also here, the Error of misclassification, Accurcacy and AUC are calculated.


#5 Experiment and Results

## 5.1 Regression

##5.1.1 Regression with the features

```{r, echo=FALSE}
dataset=data.frame("Adj.R2" =c(0.541, 0.5422, 0.4559, 0.455, 0.5524, 0.4936),
                   "RSE"=c(1.076, 1.074, 1.171, 1.172, 1.062, 1.13))
rownames(dataset)=c("LINEAR - complete", "LINEAR - reduced", "QUADRATIC - complete", "QUADRATIC - reduced",
                    "MIXED - complete", "MIXED - reduced")
knitr::kable(
  dataset, caption = 'Comparison between the fitted models'
)

```

After fitting all these models, the best two are taken into account and analyzed.

The second model (LINEAR - reduced) is:
$$ \hat{y} = 0.66 + 0.012 \cdot HomeTeam + 0.808 \cdot HTHG + 0.821\cdot HTAG + 0.161\cdot HST + 0.162\cdot AST -0.0256 \cdot AF -0.0358 \cdot HC + \epsilon $$
$$ \epsilon \sim N(0,1.074^2)$$
Between the Mixed models, the complete one should be better by looking at the adjusted R^2 but there are too many regressors and a lot of coefficients can be put as zero after performed a statistical test (p-value higher than 5%).

The MIXED - reduced model is:
$$ \hat{y} = -0.0051 + 0.7686 \cdot HTHG + 0.96898 \cdot HTAG + 0.09492 \cdot HS +  0.171 \cdot HST -0.0033 \cdot HS^2 + 0.0021 \cdot AS^2 + \epsilon$$
$$ \epsilon \sim N(0,1.1057^2)$$


```{r, echo=FALSE}
fit2 = lm ( football_train_y$FTG ~ HomeTeam + HTHG + HTAG +  HST + AST + AF + HC , data=football_train_x)
#summary(fit2)
football_prediction_FTG_2 = predict.lm(fit2, newdata=football_test_x)
ME_2=sum(football_test_y$FTG - football_prediction_FTG_2 )
ME_ratio_2= ME_2 / mean(football_test_y$FTG)
MSE_2=sum((football_test_y$FTG - football_prediction_FTG_2 )^2)
```


```{r, echo=FALSE}
fit6 = lm ( football_train_y$FTG ~ HTHG + HTAG + HS + HST + I(HS^2) + I(AS^2), data=football_train_x )
#summary(fit6)
football_prediction_FTG_6 = predict.lm(fit6, newdata=football_test_x)
ME_6=sum(football_test_y$FTG - football_prediction_FTG_6 )
ME_ratio_6= ME_6 / mean(football_test_y$FTG)
MSE_6=sum((football_test_y$FTG - football_prediction_FTG_6 )^2)
```



```{r, echo=FALSE}
dataset2=data.frame("ME" =c(72.036, 68.743),
                   "ME_ratio"=c(24.36812, 23.254),
                   "MSE"=c(501.0531, 535.198))
rownames(dataset2)=c("LINEAR - reduced","MIXED - reduced")
knitr::kable(
  dataset2, caption = 'Comparison between the two best fitted models'
)
```

In the end, the predictions for FTG given by applying the LINEAR and MIXED models to the test dataset are plotted below (on the left), respectively in BLUE and GREEN. The red points are the values of FTG in the test dataset (Y). To better understand the given results, the predicted values given by the two models can be rounded, in order to get integer values (plot on the right).

```{r, echo=FALSE, out.width='.49\\linewidth', fig.show='hold', fig.height = 4, fig.width = 4, fig.align="center"}
# PREDICTION
plot(football_prediction_FTG_2, col="blue", main="Fitted values", xlab="observations", ylab="FTG", type='p') #linear
points(football_prediction_FTG_6, col="green")
points(football_test_y$FTG, col="red")

plot(round(football_prediction_FTG_2), col="blue", main="Rounded fitted values", xlab="observations", ylab="FTG") #linear
points(round(football_prediction_FTG_6), col="green")
points(football_test_y$FTG, col="red")
```



##5.1.2 Regression with Principal Component Analysis

The principal component analysis is performed on the training dataset (x).
```{r, echo=FALSE}
pc_x = princomp(football_x, scores=T)
#summary(pc_x)
```

```{r, echo=FALSE,fig.height = 3, fig.width = 3,  fig.align="center"}
plot(cumsum(pc_x$sd^2)/sum(pc_x$sd^2), type='b', axes=F, xlab='number of components', 
     ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(football_x),labels=1:ncol(football_x),las=2)
```
By the analysis of the contribution to the cumulative variance and the rule of the 80% at least, the first four principal components are taken into account.

Linear Regression:
$$ \hat{\bar{y}} = 2.69 + 0.0104\cdot PC_1 + 0.0025\cdot PC_2 + 0.0049\cdot PC_3 - 0.108422 \cdot PC_4 +\bar{\epsilon} $$
$$ \epsilon \sim N(0,1.526^2)$$

```{r, echo=FALSE,fig.height = 3, fig.width = 3,  fig.align="center"}
scores=pc_x$scores[1:798,1:4]
colnames(scores)=c('one','two','three','four')
scores=as.data.frame(scores)

fit_pca = lm( football_train_y$FTG  ~   scores$one  + scores$two + scores$three + scores$four, data=scores  )
#summary(fit_pca)

 y= fit_pca$coefficients[1] * rep(1, 342 )+ 
  fit_pca$coefficients[2] * t(as.vector(pc_x$loadings[,1]) %*% t(football_test_x)) +
  fit_pca$coefficients[3] * t(as.vector(pc_x$loadings[,2]) %*% t(football_test_x)) +
  fit_pca$coefficients[4] * t(as.vector(pc_x$loadings[,3]) %*% t(football_test_x)) +
  fit_pca$coefficients[5] * t(as.vector(pc_x$loadings[,4]) %*% t(football_test_x))

ME=sum(football_test_y$FTG - y )
ME_ratio= ME / mean(football_test_y$FTG)
MSE=sum((football_test_y$FTG - y )^2)
```
 

Complete regression model with quadratic terms:
$$ \hat{\bar{y}} = 2.863 + 0.0122\cdot PC_1 + 0.00051\cdot PC_2 + 0.0052\cdot PC_3 - 0.11897 \cdot PC_4 + $$
$$ -0.00032 \cdot PC_1^2 -0.00136\cdot PC_2^2 -0.00176\cdot PC_3^2 +0.00098 \cdot PC_4^2 +\bar{\epsilon} $$
$$ \bar{\epsilon} \sim N(0,1.526^2)$$

```{r, echo=FALSE,fig.height = 3, fig.width = 3,  fig.align="center"}
# try with also quadratic
fit_pca = lm( football_train_y$FTG  ~   scores$one  + scores$two + scores$three + scores$four + 
                 I(scores$one^2)  + I(scores$two^2) + I(scores$three^2) + I(scores$four^2), data=scores  )
#summary(fit_pca)
#plot(fit_pca)

#dim(pc_x$loadings)
 y2= fit_pca$coefficients[1] * rep(1, 342 )+ 
  fit_pca$coefficients[2] * t(as.vector(pc_x$loadings[,1]) %*% t(football_test_x)) +
  fit_pca$coefficients[3] * t(as.vector(pc_x$loadings[,2]) %*% t(football_test_x)) +
  fit_pca$coefficients[4] * t(as.vector(pc_x$loadings[,3]) %*% t(football_test_x)) +
  fit_pca$coefficients[5] * t(as.vector(pc_x$loadings[,4]) %*% t(football_test_x)) +
  fit_pca$coefficients[6] * t(as.vector(pc_x$loadings[,5]) %*% t(football_test_x))^2 +
  fit_pca$coefficients[7] * t(as.vector(pc_x$loadings[,6]) %*% t(football_test_x))^2 +
  fit_pca$coefficients[8] * t(as.vector(pc_x$loadings[,7]) %*% t(football_test_x))^2 +
  fit_pca$coefficients[9] * t(as.vector(pc_x$loadings[,8]) %*% t(football_test_x))^2 
 
```


```{r, echo=FALSE}
dataset3=data.frame("ME" =c(-498.3534, -569.7796),
                   "ME_ratio"=c(-168.5825, -192.7444),
                   "MSE"=c(1686.886, 1910.482),
                   "R^2"=c(0.0761, 0.08004),
                   "RSE"=c(1.526, 1.523))
rownames(dataset3)=c("LINEAR - PCA","MIXED - PCA")
knitr::kable(
  dataset3, caption = 'Comparison between PCA models'
)
```

In the end, the predictions for FTG given by applying the MIXED model (with Principal Components) to the test dataset are plotted below (on the left) in orange colour. The red points are the real values of FTG in the test dataset (Y). The blue points are instead the predictions obtained by the Regression performed on all the features (in the previous section).
To better understand the given results, the predicted values given by the two models can be rounded, in order to get integer values (plot on the right).

```{r, echo=FALSE, out.width='.49\\linewidth', fig.show='hold', fig.height = 4, fig.width = 4, fig.align="center"}
# PREDICTION
plot(football_prediction_FTG_2, col="blue", main="Fitted values", xlab="observations", ylab="FTG", type='p') #linear
points(y2, col="orange")
points(football_test_y$FTG, col="red")

plot(round(football_prediction_FTG_2), col="blue", main="Rounded fitted values", xlab="observations", ylab="FTG") #linear
points(round(y2), col="orange")
points(football_test_y$FTG, col="red")
```




##5.2 Classification

##5.2.1 KNN 

##5.2.1.1 KNN with all features

```{r, echo=FALSE,out.width='.49\\linewidth', fig.show='hold', fig.height = 3, fig.width = 3, fig.align="center"}
library(class)
prediction=matrix(0,nrow=342, ncol=40)
error=rep(0,40)
for (i in 1:40) {
  prediction[,i] = knn(football_train_x, football_test_x, football_train_y$Interest, k = i)
  xtab = table(prediction[,i], football_test_y$Interest)
  error[i]=(xtab[1,2]+xtab[2,1])/342
}
#error
plot(error, ylab="Misclassification Error", main="Choice of K")
#min(error)
#which.min(error)

plot(xtab, col="lightblue", main="k=25", xlab="Confusion Matrix")
title(main = NULL, sub = NULL)
```


```{r,echo=FALSE}
# confusion matrix
prediction_25=knn(football_train_x, football_test_x, football_train_y$Interest, k = 25)
xtab = table(prediction_25, football_test_y$Interest)

knitr::kable(
  xtab, caption = 'Confusion Matrix - k=25'
)
```


$$ Misclassification \ error_{\ k=25} = 0.3626$$
$$ Accuracy_{\ k=25} = 0.6374$$


### Previous considerations on the Principal Component Analysis
Principal component analysis is performed on the whole dataset x (train and test). Data are plotted in the plane of the first two principal components, the RED points are the data for which the match is not interesting ("Interest" = 0), the BLUE ones are the interesting matches ("Interest" = 1).

```{r, echo=FALSE,out.width='.49\\linewidth', fig.show='hold', fig.height = 4, fig.width = 4, fig.align="center"}
football_x=rbind(football_train_x, football_test_x)
football_y=rbind(football_train_y, football_test_y)
pc_x = princomp(football_x, scores=T)
plot(pc_x$scores[,1:2], col=c("red","blue")[football_y$Interest+1])

plot(cumsum(pc_x$sd^2)/sum(pc_x$sd^2), type='b', axes=F, xlab='number of components', 
     ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(football_x),labels=1:ncol(football_x),las=2)
```

The cumulative variance explained by the k-th principal component is plotted. Following the rule that it should be at least 80%, the first four principal components are kept for the following analysis.


##5.2.1.2 KNN with Principal Component Analysis

Now, the optimal value k is calculated as before on this new dataset obtained after the PCA and the correspondent confusion matrix is calculated and plotted.

```{r, echo=FALSE,out.width='.49\\linewidth', fig.show='hold', fig.height = 3, fig.width = 3, fig.align="center"}
library(class)
prediction=matrix(0,nrow=342, ncol=40)
error=rep(0,40)
for (i in 1:40) {
  prediction[,i] = knn(pc_x$scores[1:798,1:4], pc_x$scores[799:1140,1:4], football_train_y$Interest, k = i)
  xtab = table(prediction[,i], football_test_y$Interest)
  error[i]=(xtab[1,2]+xtab[2,1])/342
}
#error
plot(error, ylab="Misclassification Error", main="Choice of K")
#min(error)
#which.min(error)

#print(xtab)
plot(xtab, col="lightblue", main="k=13", xlab="Confusion matrix")
```

```{r,echo=FALSE}
prediction_13=knn(pc_x$scores[1:798,1:4], pc_x$scores[799:1140,1:4], football_train_y$Interest, k = 13)
xtab = table(prediction_13, football_test_y$Interest)

knitr::kable(
  xtab, caption = 'Confusion Matrix - k=13'
)
```


$$ Misclassification \ error_{k=13} = 0.4094$$
$$ Accuracy_{k=13} = 0.5906$$




##5.2.2 Logistic Regression Classification

##5.2.2.1 Logistic Regression Classification with all features


```{r, echo=FALSE,out.width='.49\\linewidth', fig.show='hold', fig.height = 3, fig.width = 3, fig.align="center"}
model <- glm(football_train_y$Interest ~.,family=binomial(link='logit'),data=football_train_x)
fitted.results <- predict(model,newdata=football_test_x,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

xtab_logistic = table(fitted.results, football_test_y$Interest)
#print(xtab_logistic)

 knitr::kable(
  xtab_logistic, caption = 'Confusion Matrix'
)

plot(xtab_logistic, col="pink", main="Logistic regression", xlab="Confusion matrix - all features")

misClasificError <- mean(fitted.results != football_test_y$Interest)
#print(paste('Misclassification Error', misClasificError))
#print(paste('Accuracy',1-misClasificError))

p <- predict(model, newdata=football_test_x, type="response")
pr <- prediction(p,football_test_y$Interest)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, main="ROC curve")

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
#print(paste('AUC', auc))
```


$$ Misclassification \ error = 0.3333$$
$$ Accuracy = 0.66667$$
$$ AUC= 0.68895 $$





##5.2.2.2 Logistic Regression Classification with Principal Component Analysis

```{r, echo=FALSE,out.width='.49\\linewidth', fig.show='hold', fig.height = 3, fig.width = 3, fig.align="center"}
model2 <- glm(football_train_y$Interest ~.,family=binomial(link='logit'),data=as.data.frame(pc_x$scores[1:798,1:4]))
fitted.results2 <- predict(model2,newdata=as.data.frame(pc_x$scores[799:1140,1:4]),type='response')
fitted.results2 <- ifelse(fitted.results2 > 0.5,1,0)

xtab_logistic2 = table(fitted.results2, football_test_y$Interest)
#print(xtab_logistic2)
#plot(xtab_logistic2)

knitr::kable(
  xtab_logistic2, caption = 'Confusion Matrix'
)

plot(xtab_logistic2, col="pink",main="Logistic regression", xlab="Confusion matrix - P.Components")

misClasificError2 <- mean(fitted.results2 != football_test_y$Interest)
#print(paste('Misclassification Error',misClasificError2))
#print(paste('Accuracy',1-misClasificError2))

p2 <- predict(model2, newdata=as.data.frame(pc_x$scores[799:1140,1:4]), type="response")
pr2 <- prediction(p2,football_test_y$Interest)
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
plot(prf2, main="ROC curve")

auc2 <- performance(pr2, measure = "auc")
auc2 <- auc2@y.values[[1]]
#print(paste('AUC', auc2))
```



$$ Misclassification \ error = 0.4269$$
$$ Accuracy = 0.5731$$
$$ AUC = 0.53587$$


#6 Conclusion and discussion
The main results obtained in the previous section are resumed below.

##6.1 Predicting "FTG" with Regression
```{r, echo=FALSE}
dataset2=data.frame("ME" =c(72.036, 68.743, -498.3534, -569.7796),
                   "ME_ratio"=c(24.36812, 23.254, -168.5825, -192.7444 ),
                   "MSE"=c(501.0531, 535.198, 1686.886, 1910.482),
                   "R^2"=c( 0.541, 0.4936 , 0.0761, 0.08004),
                   "RSE"=c( 1.076, 1.13 , 1.526, 1.523))
rownames(dataset2)=c("LINEAR - reduced","MIXED - reduced", "LINEAR - PCA","MIXED - PCA")
knitr::kable(
  dataset2, caption = 'Comparison between the two best fitted models'
)

```

For what concern the models applied to the principal components, the resulting values are not good. The coefficients which are significatively different from zero are just $\beta_0$ and $\beta_4$, as can be seen from the R summary of the linear regression; this means that the number of goals have been predicted just by the fourth principal component (which just explains the 7.38% of the variability of the original dataset).
Moreover the value of the intercept is high (2.69 for the linear regression model and 2.863 for the complete regression model) and almost all the values of the number of goals predicted by this model are between 4 and 5. This is not a good model.

The best model seems to be the reduced linear model. The $R^2$ is highest and the MSE is the lowest.
The plots for this model are reported (as in the section 5.1.1, the RED colour is used for y and the BLUE colour for the y predicted by the model).
The only problem with this model is that it seems almost impossible to be able to predict the number of goals as 0. In fact, the only two negative coefficients are the ones referring to the variables AF and HC, but they assumes very low values (0.0256 and 0.0358) and the intercept is 0.66.

```{r, echo=FALSE, out.width='.49\\linewidth', fig.show='hold', fig.height = 4, fig.width = 4, fig.align="center"}
# PREDICTION
plot(football_prediction_FTG_2, col="blue", main="Fitted values", xlab="observations", ylab="FTG", type='p') #linear
points(football_test_y$FTG, col="red")

plot(round(football_prediction_FTG_2), col="blue", main="Rounded fitted values", xlab="observations", ylab="FTG") #linear
points(football_test_y$FTG, col="red")
```


##6.2 Predicting "Interest" with Classification

```{r, echo=FALSE, fig.height = 3, fig.width = 3, fig.align="center"}
data2=data.frame("Misclassification.Error"=c( 0.3626,  0.4094,  0.3333,  0.4269  ), "Accuracy" = c( 0.6374, 0.5906,  0.66667,  0.5731 ), "AUC"=c("-", "-", 0.68895, 0.53587 ))
rownames(data2)=c("KNN - All Features", "KNN - PCA", "Logistic Regression - All Features", "Logistic Regression - PCA")

knitr::kable(
  data2, caption = 'Comparison between Classification Models'
)

```

The best models seem to be the ones applied to all the features (instead of the ones applied on the Principal Components).
In particular, the best one is the "KNN - All Features", with a Misclassification Error of 36%. For this kind of prediction must be noticed that the probability of randomly assigning the variable "Interesting" to a match is 50%. The "KNN - PCA", for example, gives a result of 49.1%, which is defenitely not good.
Moreover, between the two models of Logistic Regression, the Misclassification Error and the AUC values are in agreement: between these two models, "Logistic Regression - All Features" model is better than the other.


#7 References

 * https://medium.com/@bioturing/how-to-read-pca-biplots-and-scree-plots-186246aae063
 * https://towardsdatascience.com/a-beginners-guide-to-linear-regression-in-python-with-scikit-learn-83a8f7ae2b4f
 * https://realpython.com/linear-regression-in-python/#python-packages-for-linear-regression
 * https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/
 * https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/
 * http://ml-tutorials.kyrcha.info/knn.html
 * https://medium.com/@bioturing/how-to-read-pca-biplots-and-scree-plots-186246aae063
 * https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5
 * https://en.wikipedia.org/wiki/Logistic_regression
